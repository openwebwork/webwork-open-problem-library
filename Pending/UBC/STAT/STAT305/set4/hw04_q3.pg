## DESCRIPTION
## Statistics 305 
## ENDDESCRIPTION

## KEYWORDS('Statistical inference; the Cauchy distribution, maximum
## likelihood estimation, Newton-Raphson method; evaluating the log-likelihood
## function for a Cauchy distribution, finding the first derivative of the
## log-likelihood for a Cauchy distribution and evaluating it at the sample
## median, finding the second derivative of the log-likelihood for a Cauchy
## distribution and evaluating it at the sample median, approximating the MLE
## of the location parameter of a Cauchy distribution via two iterations of the
## Newton-Raphson method.')

## DBsubject('Statistics')
## DBchapter('Point Estimation')
## DBsection('Maximum Likelihood Estimation') 
## level(4)
## Date('2014/06/18')
## Author('Andy Leung')
## Institution('University of British Columbia')
## TitleText1('No Text')
## EditionText1('?')
## AuthorText1('?')
## Section1('?')
## Problem1('?')



########################################################################

DOCUMENT();      

loadMacros(
  "PGstandard.pl",
  "PGchoicemacros.pl",
  "PGcourse.pl",
  "parserRadioButtons.pl",
  "MathObjects.pl",
  "parserMultiAnswer.pl",
  "unionTables.pl",
  "RserveClient.pl",
  "parserPopUp.pl"
);

# Print problem number and point value (weight) for the problem
TEXT(beginproblem());

##############################################################
#  Question and R Setup
##############################################################
Context->("Numeric");

# Do this in R
rserve_start();

# Randomly generate parameters:
@x = rserve_eval('x <- round(rcauchy(10, location=50),1); x');
$x = join(", ", @x);

@mu = rserve_eval('mu <- round(runif(1, min=45, max=55),1); mu');
$mu = @mu[0];

@q1ans = rserve_eval('round( -10*log(pi) - sum(log( 1 + (x - mu)^2 ) ), 3)');
$q1ans = @q1ans[0];

@xmed = rserve_eval('xmed <- median(x)');
$xmed = join(", ", @xmed);

@q2ans = rserve_eval('mu <- xmed; l1 <- 2*sum((x-mu)/(1+(x-mu)^2)); round(l1,3)');
$q2ans = @q2ans[0];

@q3ans = rserve_eval('l11 <- 2*sum(((x-mu)^2-1)/(1+(x-mu)^2)^2); round(l11,3)');
$q3ans = @q3ans[0];

@q4ans = rserve_eval(' mu1 <- median(x) - l1/l11; round( mu1, 3)');
$q4ans = @q4ans[0];

@q5ans = rserve_eval(' 
l1 <- 2*sum((x-mu1)/(1+(x-mu1)^2));
l11 <- 2*sum(((x-mu1)^2-1)/(1+(x-mu1)^2)^2);
mu2 <- mu1 - l1/l11; round( mu2, 3)
');
$q5ans = @q5ans[0];

rserve_finish();


# Multiple choice question for part a
$qu1new = "Which of the following is true for the density function \(f\left( x;\mu
\right)\)?";
$ans1new = "\(f\left( x;\mu \right) =f\left( x-\mu ;0\right)\)";
$fans1_1new = "\(f\left( x;0\right) =f\left( 0;\mu \right)\)";
$fans1_2new = "\(f\left( x;\mu \right) =f\left( \mu -x;\mu \right)\)";
$fans1_3new = "\(f\left( x;\mu \right) =f\left( x;x-\mu \right)\)";
$fans1_4new = "\(f\left( x;\mu \right) =f\left( -x;\mu \right)\)";

$mc1new = new_multiple_choice();
$mc1new->qa(
  $qu1new,
  $ans1new
);
$mc1new->extra(
$fans1_1new,$fans1_2new,$fans1_3new,$fans1_4new
);


##############################################################




##############################################################
#  Question in Text
##############################################################
Context()->texStrings;
BEGIN_TEXT

A random sample of ten observations is taken from the distribution
with density function
\begin{align*}
f\left( x;\mu \right) =\frac{1}{\pi \left( 1+\left( x-\mu \right)
^{2}\right) },
\end{align*}
defined for \(-\infty <x<\infty\), with parameter \(\mu\). We seek to estimate 
\(\mu\) by maximum likelihood estimation. Sketch the above density function
for at least one value of \(\mu\), and verify that the parameter indicates
the location of the distribution. The data observed are as follows:
\begin{align*}
$x.
\end{align*}
In the following, work to at least four decimal places throughout and
provide all answers to three decimal places.
$BR
$BR

$BBOLD Part a) $EBOLD
\{ $mc1new->print_q() \}
$BR
\{ $mc1new->print_a() \}
$BR
$BR


$BBOLD Part b) $EBOLD 
Write down the log-likelihood function in this case for a general data
set \(x_{1},...,x_{n}\). Evaluate the log-likelihood function for the data
given at the point \(\mu = $mu\).
$BR
\{ ans_rule(12) \}
$BR
$BR

$BBOLD Part c) $EBOLD 
For maximum likelihood estimation we seek the parameter value \(\theta\)
that maximises the (log-)likelihood; for this, you have differentiated the
log-likelihood \(l\left( \theta \right)\), set the resulting expression to
zero, and solved for \(\theta\). Although solving \(l^{\prime}\left( \theta
\right) =0\) often leads to a simple expression for the maximum likelihood
estimator, it is not guaranteed to do so. You should verify that for the
density function \(f\left( x;\mu \right)\) above, there is no algebraic
solution to \(l^{\prime}\left( \mu \right) =0\). In such cases we may adopt
an $BITALIC iterative $EITALIC approach. One such, the Newton-Raphson method, is based
on a Taylor series approximation to the first derivative of the
log-likelihood:
\begin{align*}
l^{\prime }\left( \theta \right) \approx l^{\prime }\left( \tau \right) +\left(
\theta -\tau \right) l^{\prime \prime }\left( \tau \right) .
\end{align*}
Since we seek the value of \(\theta\) such that \(l^{\prime }\left( \theta
\right) =0\), it follows that
\begin{align*}
\theta \approx \tau -\frac{l^{\prime }\left( \tau \right) }{l^{\prime \prime }\left(
\tau \right) }.
\end{align*}
If the value \(\tau\) is an initial guess at the solution to \(l^{\prime }\left( \theta
\right) =0\), we can iterate via the following algorithm
\begin{align*}
\theta _{k+1}=\theta _{k}-\frac{l^{\prime }\left( \theta _{k}\right) }{%
l^{\prime \prime }\left( \theta _{k}\right) },
\end{align*}
for \(k=0,1,...\), taking \(\theta _{0}=\tau\). In this way we can hope to
converge to the maximum likelihood estimate in several iterations if a good
choice is made for the starting point \(\theta _{0}\). In our case here, the
sample median can provide a good initial estimate. Differentiate the
log-likelihood function, and evaluate it for the data given at the point 
\(\mu = $xmed\).
$BR
\{ ans_rule(12) \}
$BR
$BR


$BBOLD Part d) $EBOLD 
Find the second derivative of the log-likelihood, and evaluate it at
the point \(\mu = $xmed\), where $BBOLD x $EBOLD is the observed data. 
$BR
\{ ans_rule(12) \}
$BR
$BR


$BBOLD Part e) $EBOLD 
Hence find \(\mu_{1}\), the estimate from the first iteration of the
Newton-Raphson method.
$BR
\{ ans_rule(12) \}
$BR
$BR

$BBOLD Part f) $EBOLD 
Using your estimate from (e), find the estimate \(\mu _{2}\) from the
second iteration of the Newton-Raphson method.
$BR
\{ ans_rule(12) \}
$BR
$BR

END_TEXT
##############################################################



##############################################################
## Show answer
##############################################################
$showPartialCorrectAnswers = 1;
ANS( radio_cmp($mc1new->correct_ans()) );
ANS( num_cmp($q1ans, tol=> 0.001) );
ANS( num_cmp($q2ans, tol=> 0.001) );
ANS( num_cmp($q3ans, tol=> 0.001) );
ANS( num_cmp($q4ans, tol=> 0.001) );
ANS( num_cmp($q5ans, tol=> 0.001) );


##############################################################



##############################################################
#  Solution
##############################################################
Context()->texStrings;
Context()->{format}{number} = "%.2f";
BEGIN_SOLUTION

$BBOLD Part a) $EBOLD 
The density function is symmetric about \(\mu\),
hence \(f\left( x;\mu \right) =f\left( x-\mu ;0\right)\).
$BR
$BR

$BBOLD Part b) $EBOLD 
The likelihood function in general is
\begin{align*}
L\left( \mu; x_{1},\dots ,x_{n}\right) =\frac{1}{\pi^{n}}\prod_{i=1}^{n}%
\frac{1}{1+\left( x_{i}-\mu \right) ^{2}}.
\end{align*}
The log-likelihood is therefore
\begin{align*}
l\left( \mu \right) =-n\log \left( \pi \right) -\sum_{i=1}^{n}\log \left(
1+\left( x_{i}-\mu \right) ^{2}\right) .
\end{align*}
This can be found in R (with the data in vector $BITALIC x $EITALIC) at the
point \(\mu = $mu\) via
$BR
$BR
$BITALIC
mu <- $mu $BR
x <- c($x) $BR
-10*log(pi)-sum(log(1+(x-mu)^2)) 
$EITALIC
$BR
$BR



$BBOLD Part c) $EBOLD 
The first derivative of the log-likelihood is 
\begin{align*}
l^{\prime }\left( \mu \right) =2\sum_{i=1}^{10}\frac{\left( x_{i}-\mu
\right) }{\left( 1+\left( x_{i}-\mu \right) ^{2}\right) }.
\end{align*}
Evaluating the above at the median of the data can be done in R via
$BR
$BR
$BITALIC
mu0 <- median(x) $BR
l1 <- 2*sum((x-mu0)/(1+(x-mu0)^2))
$EITALIC
$BR
$BR



$BBOLD Part d) $EBOLD 
The second derivative is a little messy - differentiating
\(l^{\prime }\left( \mu \right)\) we see
\begin{align*}
l^{\prime \prime }\left( \mu \right)  &= 2\sum_{i=1}^{10}\left( -\frac{1}{
1+\left( x_{i}-\mu \right) ^{2}}+\frac{2\left( x_{i}-\mu \right) ^{2}}{
\left( 1+\left( x_{i}-\mu \right) ^{2}\right) ^{2}}\right) \left( \frac{
2\left( x_{i}-\mu \right) ^{2}-1-\left( x_{i}-\mu \right) ^{2}}{\left(
1+\left( x_{i}-\mu \right) ^{2}\right) ^{2}}\right)  \\
&= 2\sum_{i=1}^{10}\frac{\left( x_{i}-\mu \right) ^{2}-1}{\left( 1+\left(
x_{i}-\mu \right) ^{2}\right) ^{2}}.
\end{align*}
This can be found in R at \(\mu = $xmed\) with
$BR
$BR
$BITALIC
l11 <- 2*sum(((x-mu0)^2-1)/(1+(x-mu0)^2)^2)
$EITALIC
$BR
$BR


$BBOLD Part e) $EBOLD 
 We are now in a position to apply the first iteration of the
algorithm, to give
\begin{align*}
\mu _{1}=$xmed -\frac{l^{\prime }\left( $xmed
\right) }{l^{\prime \prime }\left( $xmed\right) }= $q4ans.
\end{align*}
In R, this is 
$BR
$BR
$BITALIC
mu1 <- mu0 - l1/l11
$EITALIC
$BR
$BR

$BBOLD Part f) $EBOLD 
The estimate from the second iteration is 
\begin{align*}
\mu _{2}=\mu _{1}-\frac{l^{\prime }\left( \mu _{1}\right) }{l^{\prime \prime
}\left( \mu _{1}\right) }= $q5ans.
\end{align*}
In R, 
$BR
$BR
$BITALIC
l1 <- 2*sum((x-mu1)/(1+(x-mu1)^2))') $BR
l11 <- 2*sum(((x-mu1)^2-1)/(1+(x-mu1)^2)^2) $BR
mu2 <- mu1 - l1/l11
$EITALIC
$BR
$BR
$BR

$BITALIC
You are encouraged to continue the algorithm until convergence is
achieved, which could be taken to be when consecutive estimates are equal in
the first three decimal places. You can plot the (log-)likelihood function
to verify that the maximum has been attained. 
$BR

This distribution is known as the Cauchy distribution, and for small
sample sizes the likelihood function is often not unimodal. In such cases
the algorithm may fail due to converging to a local (but not global)
maximum. The Cauchy distribution is unusual in other respects, notably in
that it has no moments. Hence standard theory for maximum likelihood
estimators does not apply.  
$EITALIC


END_SOLUTION
##############################################################



ENDDOCUMENT();